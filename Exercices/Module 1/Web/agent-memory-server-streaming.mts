import { BaseMessage, HumanMessage, SystemMessage } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { MemorySaver, START, StateGraph } from "@langchain/langgraph";
import { ToolNode, toolsCondition } from "@langchain/langgraph/prebuilt";
import { ChatOpenAI } from "@langchain/openai";
import "dotenv/config";
import express from "express";
import { z } from "zod";

// ============================================================================
// CONFIGURATION DES OUTILS
// ============================================================================

const addTool = tool(
  async ({ a, b }) => {
    return a + b;
  },
  {
    name: "add",
    description: "Additionne deux nombres entiers",
    schema: z.object({
      a: z.number().describe("Premier nombre entier"),
      b: z.number().describe("DeuxiÃ¨me nombre entier"),
    }),
  }
);

const multiplyTool = tool(
  async ({ a, b }) => {
    return a * b;
  },
  {
    name: "multiply",
    description: "Multiplie deux nombres entiers",
    schema: z.object({
      a: z.number().describe("Premier nombre entier"),
      b: z.number().describe("DeuxiÃ¨me nombre entier"),
    }),
  }
);

const divideTool = tool(
  async ({ a, b }) => {
    return a / b;
  },
  {
    name: "divide",
    description: "Divise deux nombres entiers",
    schema: z.object({
      a: z.number().describe("Dividende"),
      b: z.number().describe("Diviseur"),
    }),
  }
);

const tools = [addTool, multiplyTool, divideTool];

// ============================================================================
// CONFIGURATION DU MODÃˆLE ET DE L'AGENT
// ============================================================================

if (!process.env.OPENAI_API_KEY) {
  throw new Error("OPENAI_API_KEY n'est pas dÃ©fini dans l'environnement");
}

const llm = new ChatOpenAI({ model: "gpt-4o", streaming: true });
const llmWithTools = llm.bindTools(tools);

const sysMsg = new SystemMessage({
  content: "You are a helpful assistant tasked with performing arithmetic on a set of inputs.",
});

// Ã‰tat avec messages
type MessagesState = {
  messages: BaseMessage[];
};

function addMessages(current: BaseMessage[], update: BaseMessage | BaseMessage[]): BaseMessage[] {
  const messagesToAdd = Array.isArray(update) ? update : [update];
  return [...current, ...messagesToAdd];
}

// NÅ“ud assistant (sans streaming - utilisÃ© par le graphe)
async function assistant(state: MessagesState) {
  const result = await llmWithTools.invoke([sysMsg, ...state.messages]);
  return { messages: [result] };
}

// Construction du graphe avec mÃ©moire
const builder = new StateGraph<MessagesState>({
  channels: {
    messages: {
      default: () => [],
      reducer: addMessages,
    },
  },
});

builder.addNode("assistant", assistant);
builder.addNode("tools", new ToolNode(tools));
builder.addEdge(START as any, "assistant" as any);
builder.addConditionalEdges("assistant" as any, toolsCondition as any);
builder.addEdge("tools" as any, "assistant" as any);

// Compilation avec mÃ©moire
const memory = new MemorySaver();
const reactGraphMemory = builder.compile({ checkpointer: memory });

// ============================================================================
// SERVEUR WEB EXPRESS
// ============================================================================

const app = express();
const PORT = process.env.PORT || 3001;

// Middleware pour parser le JSON
app.use(express.json());

// Endpoint pour discuter avec l'agent (streaming SSE)
app.post("/chat", async (req, res) => {
  try {
    const { message, thread_id } = req.body;

    // Validation de l'input
    if (!message || typeof message !== "string") {
      return res.status(400).json({
        error: "Le champ 'message' est requis et doit Ãªtre une chaÃ®ne de caractÃ¨res",
      });
    }

    // GÃ©nÃ©rer un thread_id si non fourni
    const threadId = thread_id || `thread_${Date.now()}`;
    const config = { configurable: { thread_id: threadId } };

    console.log(`[${new Date().toISOString()}] Thread: ${threadId}, Message: ${message}`);

    // Configurer les headers pour Server-Sent Events (SSE)
    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    // Envoyer le thread_id au dÃ©but
    res.write(`data: ${JSON.stringify({ type: "thread_id", thread_id: threadId })}\n\n`);

    // RÃ©cupÃ©rer l'Ã©tat actuel du thread pour avoir le contexte
    let existingMessages: BaseMessage[] = [];
    try {
      const currentState = await reactGraphMemory.getState(config);
      if (currentState?.values?.messages && Array.isArray(currentState.values.messages)) {
        existingMessages = currentState.values.messages;
      }
    } catch (error) {
      // Nouveau thread, pas de messages existants
      console.log("Nouveau thread, aucun Ã©tat existant");
    }

    // PrÃ©parer les messages avec le contexte complet du thread
    const allMessages = [...existingMessages, new HumanMessage({ content: message })];

    // VRAI STREAMING TOKEN PAR TOKEN directement depuis le LLM
    let aiMessage: any = null;
    
    const stream = await llmWithTools.stream([sysMsg, ...allMessages]);
    
    for await (const chunk of stream) {
      // Accumuler le message complet
      if (!aiMessage) {
        aiMessage = chunk;
      } else {
        aiMessage = aiMessage.concat(chunk);
      }
      
      // Stream des tokens de contenu en temps rÃ©el
      if (chunk.content && typeof chunk.content === "string" && chunk.content.length > 0) {
        res.write(`data: ${JSON.stringify({ 
          type: "token", 
          content: chunk.content
        })}\n\n`);
      }
    }

    // VÃ©rifier si le message a des tool_calls
    const hasToolCalls = aiMessage && "tool_calls" in aiMessage && aiMessage.tool_calls && aiMessage.tool_calls.length > 0;

    if (hasToolCalls) {
      // Envoyer les tool_calls dÃ©tectÃ©s
      res.write(`data: ${JSON.stringify({ 
        type: "tool_call", 
        tools: aiMessage.tool_calls.map((tc: any) => ({
          name: tc.name,
          args: tc.args
        }))
      })}\n\n`);

      // ExÃ©cuter le graphe complet pour gÃ©rer les outils
      for await (const event of await reactGraphMemory.stream(
        {
          messages: [new HumanMessage({ content: message })],
        },
        {
          ...config,
          streamMode: "updates" as const,
        }
      )) {
        const eventData = event as any;
        
        // Envoyer les rÃ©sultats des outils
        if (eventData.tools) {
          const toolMsgs = eventData.tools.messages;
          for (const toolMsg of toolMsgs) {
            if (toolMsg._getType() === "tool") {
              res.write(`data: ${JSON.stringify({ 
                type: "tool_result", 
                content: toolMsg.content
              })}\n\n`);
            }
          }
        }
        
        // Stream la rÃ©ponse finale token par token
        if (eventData.assistant) {
          const finalMessages = eventData.assistant.messages;
          const lastAiMsg = finalMessages[finalMessages.length - 1];
          
          if (lastAiMsg._getType() === "ai" && lastAiMsg.content) {
            // RÃ©cupÃ©rer l'Ã©tat complet avant de streamer la rÃ©ponse finale
            const updatedState = await reactGraphMemory.getState(config);
            const finalStream = await llm.stream([sysMsg, ...updatedState.values.messages]);
            
            for await (const chunk of finalStream) {
              if (chunk.content && typeof chunk.content === "string" && chunk.content.length > 0) {
                res.write(`data: ${JSON.stringify({ 
                  type: "token", 
                  content: chunk.content
                })}\n\n`);
              }
            }
          }
        }
      }
    } else {
      // Pas de tool calls, sauvegarder la rÃ©ponse dans le graphe
      await reactGraphMemory.invoke(
        {
          messages: [new HumanMessage({ content: message })],
        },
        config
      );
    }

    // RÃ©cupÃ©rer l'Ã©tat final pour envoyer le nombre total de messages
    const state = await reactGraphMemory.getState(config);
    
    // Envoyer un Ã©vÃ©nement de fin
    res.write(`data: ${JSON.stringify({ 
      type: "done", 
      message_count: state.values.messages.length 
    })}\n\n`);
    
    res.end();
  } catch (error) {
    console.error("Erreur lors du traitement de la requÃªte:", error);
    res.write(`data: ${JSON.stringify({ 
      type: "error", 
      error: error instanceof Error ? error.message : String(error) 
    })}\n\n`);
    res.end();
  }
});

// DÃ©marrage du serveur
app.listen(PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ¤– Agent Memory Server (STREAMING) dÃ©marrÃ© avec succÃ¨s  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  URL:        http://localhost:${PORT}                         â•‘
â•‘  Streaming:  POST http://localhost:${PORT}/chat               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ¨ Streaming SSE activÃ© pour des rÃ©ponses en temps rÃ©el !
  `);
});
